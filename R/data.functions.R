# Functions for handling data
# Author: Hugo Pedder
# Created: 2023-03-27


#' Initialise directory
#'
#' Creates relevant subfolders
#'
#' @param wd A path to the directory which will be the source for the analyses
#'
#' @export
init.dir <- function(wd=NULL) {

  checkmate::assertCharacter(wd, null.ok=TRUE,
                             overwrite=FALSE)

  storewd <- getwd()

  setwd(wd)

  folders <- c("Results",
               "Results/Codas",
               "Plots",
               "BUGSdata",
               "BUGSresults",
               "BUGSmodels",
               "JAGSmodels")
  for (i in seq_along(folders)) {
    if (!dir.exists(folders[i])) {
      dir.create(folders[i])
    }
  }

  if (!file.exists("BUGSresults/bugsresults.xlsx")) {
    wb <- openxlsx::createWorkbook(title="BUGSResults")
    openxlsx::addWorksheet(wb, sheet="Intro")
    openxlsx::writeData(wb, sheet="Intro", x=c("This file contains all the posterior summary statistics from different analyses",
                                               "Each analysis should be saved in a new tab"))
    openxlsx::saveWorkbook(wb, file="BUGSresults/bugsresults.xlsx", overwrite=overwrite)
  } else {
    warning("bugsresults.xlsx already exists and so will not be overwritten")
  }

  setwd(storewd)
}






# Convert df columns to numeric (after checking removal of NAs)

#' Convert columns of a data frame to numeric (after checking removal of NAs)
#' @param df A data frame
#' @param col A numeric vector representing column indices in `df` to convert to numeric
#' @param drop.ws Whether whitespaces should be dropped
#'
#' @export
convert.numeric <- function(df, col, drop.ws=TRUE) {

  for (i in seq_along(col)) {

    locstr <- paste0(col[i], "\nVariable name: ", names(df)[col[i]])

    # Remove NAs
    df[[col[i]]] <- gsub("( +)?na( +)?", "NA", df[[col[i]]])

    # Check for white spaces
    if (any(grepl(" ", df[[col[i]]]))) {

      if (drop.ws==TRUE) {
        df[[col[i]]] <- gsub(" ", "", df[[col[i]]])
        warning(paste0("Whitespace in column", locstr))
      } else {
        stop(paste0("Whitespace in column", locstr))
      }
    }

    # Check for non-NA string
    if (any(grepl("[A-z]", df[[col[i]]][df[[col[i]]]!="NA"]))) {
      stop(paste0("Non-NA string in column ", locstr))
    }

    # As numeric
    df[[col[i]]] <- suppressWarnings(as.numeric(df[[col[i]]]))
  }

  return(df)
}






# Uses box-cox approach

#' Converts medians to means using Box-Cox transformation
#'
#' @param df A dataframe in which continuous variables are labelled with the following
#' information (each separated by an underscore (_)):
#'  * scale: the name of the measurement scale (e.g. "beck") (if `scale=NULL` then this isn't necessary)
#'  * measurement: the time-point of interest (e.g. "cfb", "base", "final")
#'  * moment: the moment required for data transformation. Moments required in `df` for
#'  transformation are `median`, `iqrlow`, `iqrhigh`.
#'
#' @details
#' An example of the variables included in `df` may be `"beck_cfb_median"`, or
#' `"hamd_base_iqrlow"`. Means and SDs estimated using this function will then be stored as new
#' variables (or will overwrite existing ones) corresponding to `scale_measurement_mean` and `scale_measurement_sd`
#'
#' @export
medtomean <- function(df, scale=NULL, measurement="base") {

  if (!is.null(scale)) {
    varstart <- paste(scale, measurement, sep="_")
  } else {
    varstart <- measurement
  }

  # Check for presence of necessary variables in df
  moments <- c("median", "iqrlow", "iqrhigh")
  for (i in seq_along(moments)) {
    temp <- paste(varstart, moments[i], sep="_")
    if (!temp %in% names(df)) {
      stop(paste0(temp, " is not a variable in df but must be present for estimation of specified mean/sd"))
    }
  }

  moments <- c("mean", "sd")
  for (i in seq_along(moments)) {
    temp <- paste(varstart, moments[i], sep="_")
    if (!temp %in% names(df)) {
      warning(paste0(temp, " is not a variable in df - it will be generated by medtomean()"))
    }
  }


  for (i in 1:nrow(df)) {

    skip <- FALSE

    # If median data is present
    if (!is.na(df[[paste(varstart, "median", sep="_")]][i])) {

      if (measurement=="base" | df$itt[i]=="itt") {
        n <- df$n_randomised[i]
      } else {
        n <- df$n_completers[i]
      }

      if (df[[paste(varstart, "median", sep="_")]][i]==0 &
          df[[paste(varstart, "iqrlow", sep="_")]][i]==0) {
        skip <- TRUE
      }


      if (skip==FALSE) {

        if (any(c(df[[paste(varstart, "median", sep="_")]][i],
                  df[[paste(varstart, "iqrlow", sep="_")]][i],
                  df[[paste(varstart, "median", sep="_")]][i])<0)) {

          # Use QE method if any values are negative (assumes normality)
          temp <- qe.mean.sd(q1.val = df[[paste(varstart, "iqrlow", sep="_")]][i],
                             q3.val = df[[paste(varstart, "iqrhigh", sep="_")]][i],
                             med.val = df[[paste(varstart, "median", sep="_")]][i],
                             n=n)
        } else {

          # Otherwise use Box-Cox method
          temp <- bc.mean.sd(q1.val = df[[paste(varstart, "iqrlow", sep="_")]][i],
                             q3.val = df[[paste(varstart, "iqrhigh", sep="_")]][i],
                             med.val = df[[paste(varstart, "median", sep="_")]][i],
                             n=n)
        }


        # If mean data is missing
        if (!is.na(df[[paste(varstart, "mean", sep="_")]][i])) {

          df[[paste(varstart, "mean", sep="_")]][i] <- temp$est.mean
          message(paste0("Estimated mean for ", df$study_id[i], ": ", temp$est.mean))
        }

        # If sd data is missing
        if (!is.na(df[[paste(varstart, "sd", sep="_")]][i])) {

          df[[paste(varstart, "sd", sep="_")]][i] <- temp$est.sd
          message(paste0("Estimated SD for ", df$study_id[i], ": ", temp$est.sd))
        }
      }
    }
  }
  return(df)
}






# Used for renumbering a vector
renumber <- function(x) {
  y <- vector()
  uni <- unique(x)
  for (i in 1:(length(uni))) {
    y <- append(y, rep(i, sum(x==uni[i])))
  }
  warning("Function only works if reference changed is the only treatment in its class\nCheck renumber() has worked!")
  return(y)
}





#' Clean continuous data for modelling using standardised mean differences (SMD)
#'
#' Cleans continuous data reported on multiple scales for modelling using SMDs. Also generates
#' treatment/class codes required for analysis.
#'
#' @param df A cleaned data frame containing variables specified in y.b, y.f, y.cfb, sd.b, sd.f, sd.cfb, n.b and n.f.
#' Must also include the following variables:
#' * `studyid`: study identifiers
#' * `trt`: treatment names/identifiers corresponding to those in `classcode`
#' * `class`: class names/identifiers corresponding to those in `classcode` (if classes are modelled)
#' @param classcode A data frame of treatment/class identifiers
#' @param y.b A string corresponding to the variable in `df` for the mean at baseline
#' @param y.f A string corresponding to the variable in `df` for the mean at follow-up
#' @param y.cfb A string corresponding to the variable in `df` for the mean change from baseline
#' @param sd.b A string corresponding to the variable in `df` for the standard deviation at baseline
#' @param sd.f A string corresponding to the variable in `df` for the standard deviation at follow-up
#' @param sd.cfb A string corresponding to the variable in `df` for the standard deviation change from baseline
#' @param n.b A string corresponding to the variable in `df` for the number of participants randomised
#' @param n.f A string corresponding to the variable in `df` for the number of completers
#' @param scale A string corresponding to the variable in `df` that reports the measurement scale (e.g. HAM-D).
#' `cleancont.smd` assumes the use of SMDs is because multiple measurement scales are present. If only a single
#' scale is present then can create a variable in `df` that gives the same value for all rows of data.
#' @param predsd Indicates whether missing SDs should be predicted (`TRUE`) using other reported SDs (baseline,
#' follow-up and CFB) on the same measurement scale. If `TRUE`, plots and regression results for predictions
#' will be generated in addition to the cleaned data.
#' @param connectonly Indicates whether treatments only used for connecting should be included (`TRUE`). Checks for
#' by excluding treatments based on `classcode`, specified as `classcode$connectonly==1`
#' @param drop.studies A vector of study names to be excluded from the analysis that
#' correspond to those given in `df$study_id`
#' @param connect.study A character vector of study identifiers matching those in `df$study_id`
#' that should be included specifically for network connectivity. If a study in `connect.study`
#' includes a treatment specified as `classcode$connectonly==1` (and if `connectonly=TRUE`) then
#' the study will be included (i.e. `connect.study` takes priority over `connectonly`).
#' @param ref An index for which treatment should become the reference treatment in `classcode`.
#'
#' @export
cleancont.smd <- function(df, classcode,
                      y.b, y.f, y.cfb,
                      sd.b, sd.f, sd.cfb,
                      n.b, n.f,
                      scale,
                      predsd=TRUE,
                      connectonly=FALSE, drop.studies=NULL, drop.trt=NULL,
                      connect.study=c(), ref=NULL) {

  # Identify scale
  # scale <- gsub("^([A-z]+)(\\_.+)(\\_.+)", "\\1", y.cfb)
  # scale <- paste0(scale, "_scale.x")
  df$scale <- df[[scale]]

  # Create variables for analysis
  vars <- c("y.b", "y.f", "y.cfb",
            "sd.b", "sd.f", "sd.cfb")
  for (i in seq_along(vars)) {
    df[[vars[i]]] <- df[[get(vars[i])]]
  }

  # Drop specified studies
  if (!is.null(drop.studies)) {
    df <- df[!df$study %in% c(drop.studies),]
  }


  # Select only studies that have the correct info required for transformation to SMD
  # THIS WAS USED IN DEPRESSION BUT NOT RELEVANT HERE
  # if (transform==TRUE) {
  #
  #   dropmat <-
  #     is.na(df[,paste0("b.itt_", c(1:5))]) | is.na(df[,paste0("sd.b.itt_", c(1:5))]) | is.na(df[,paste0("resp.q_", c(1:5))])
  #   keepmat <- !dropmat
  #
  #   rmat <- df[,paste0(r, "_", c(1:5))]
  #   rmat <- ifelse(keepmat==FALSE, NA, keepmat) * rmat
  #   df[,paste0(r, "_", c(1:5))] <- rmat
  #
  #   nmat <- df[,paste0(n, "_", c(1:5))]
  #   nmat <- ifelse(keepmat==FALSE, NA, keepmat) * nmat
  #   df[,paste0(n, "_", c(1:5))] <- nmat
  #
  #   drops <- apply(dropmat, MARGIN=1, FUN=function(x) {all(x==TRUE)})
  #   print("`transform=TRUE; Studies do not have available data for transform:")
  #   print(paste(df$study[drops], sep="\n"))
  #
  #   df <- df[!drops,]
  # }


  # Drop specified treatments
  for (i in seq_along(drop.trt)) {
    df <- subset(df, trt!=drop.trt[i])
  }


  # Select outcome
  # Drop studies with missing data on follow-up or CFB
  df <- subset(df, !(is.na(df$y.f) & is.na(df$y.cfb)))

  # Create variable to indicate which N should be used (prefer n.f over n.b)
  df$usen <- ifelse(is.na(df[[n.f]]), n.b, n.f)
  df$n <- ifelse(df$usen==n.b, df[[n.b]], df[[n.f]])

  # Drop if n is na
  drops <- which(is.na(df$n))
  if (length(drops>0)) {
    df <- df[-drops,]
    message(paste0("Studies have missing data on n: ", paste0(df$study_id[drops], collapse=", ")))
  }

  # Create variable to indicate which Y should be used (prefer CFB over F)
  df$usecont <- ifelse(!is.na(df$y.cfb) & !is.na(df$sd.cfb) & !is.na(df$n),
                       y.cfb,
                       y.f)

  # If CFB check that required data is present
  if (predsd==TRUE) {
    isna <- df$usecont==y.cfb & is.na(df$sd.cfb) & is.na(df$sd.b) & is.na(df$sd.f)
    if (any(isna==TRUE)) {

      # Predict sd.cfb from other studies

      predsd.df <- subset(df, !is.na(sd.cfb))

      # Predict missing SDs for CFB (using scale available)
      predsd.df$logcfb <- log(predsd.df$sd.cfb)
      fit <- lm(data=predsd.df, logcfb ~ scale) # Predict sd.b from scale only
      print(summary(fit))

      sub <- df[isna==TRUE,]
      print(table(sub$scale))

      logcfb <- predict(fit, newdata = sub)

      message(paste0("CFB SD imputed using scale for: ",
                   paste0(unique(sub$study_id), collapse=", ")))

      # Add predictions
      df$sd.cfb[is.na(df$sd.cfb)] <-
        exp(logcfb)
    }
  }

  # If BF check that y.b and y.f are present
  isna <- df$usecont==y.f & is.na(df$y.b)
  if (any(isna==TRUE)) {
    stop(paste0("Studies are missing data on y.b: ", paste0(df$study_id[isna], collapse=", ")))
  }


  # If BF check that sd.b and sd.f are present
  isna <- df$usecont==y.f & (is.na(df$sd.b) | is.na(df$sd.f))
  if (any(isna==TRUE)) {
    if (predsd==TRUE) {

      predsd.df <- subset(df, !is.na(sd.b) & !is.na(sd.f))

      # Predict missing SDs for BF baseline data (using scale available)
      g <- ggplot(data=predsd.df, aes(x=log(sd.b), y=log(sd.f),
                                      color=scale)) +
        geom_point()

      predsd.df$logf <- log(predsd.df$sd.f)
      predsd.df$logb <- log(predsd.df$sd.b)
      fit <- lm(data=predsd.df, logb ~ scale) # Predict sd.b from scale only
      print(summary(fit))

      sub <- subset(df, is.na(df$sd.b))
      print(table(sub$scale))

      logb <- predict(fit, newdata = sub)

      message(paste0("Baseline SD imputed using scale for: ",
                   paste0(unique(sub$study_id), collapse=", ")))

      # Add predictions
      df$sd.b[is.na(df$sd.b)] <-
        exp(logb)



      # Predict missing SDs for BF data (using sd.f and scale if available)
      g <- ggplot(data=predsd.df, aes(x=log(sd.b), y=log(sd.f),
                                      color=scale)) +
        geom_point()

      predsd.df$logf <- log(predsd.df$sd.f)
      predsd.df$logb <- log(predsd.df$sd.b)
      fit <- lm(data=predsd.df, logf ~ logb + scale) # Predict sd.f from sd.b
      print(summary(fit))

      sub <- subset(df, is.na(df$sd.f) & !is.na(df$sd.b))
      sub$logb <- log(sub$sd.b)
      print(table(sub$scale))

      logf <- predict(fit, newdata = sub)

      message(paste0("Followup SD imputed using scale and sd.b for: ",
                   paste0(unique(sub$study_id), collapse=", ")))

      # Add predictions
      df$sd.f[is.na(df$sd.f) & !is.na(df$sd.b)] <-
        exp(logf)

    } else {
      stop(paste0("Studies are missing data on sd.b or sd.f: ", paste0(df$study_id[isna], collapse=", ")))
    }
  }


  # If sd.cfb is missing, estimate from sd.b and sd.f correlation
  isna <- !is.na(df$y.cfb) & is.na(df$sd.cfb)
  if (any(isna==TRUE)) {
    if (predsd==TRUE) {

      # Estimate correlation between y.f and y.b using complete dataset
      predsd.df <- df[!is.na(df$sd.cfb) & !is.na(df$sd.b) & !is.na(df$sd.f),]

      var.b <- (predsd.df$sd.b^2)
      var.f <- (predsd.df$sd.f^2)
      var.cfb <- (predsd.df$sd.cfb^2)

      # COV(y,x) = (VAR(y) + VAR(x) - VAR(y-x) ) /2
      cov.x.y <- (var.b + var.f - var.cfb) / 2
      cor.x.y <- cov.x.y / ((var.b^0.5) * (var.f^0.5))

      cor <- quantile(cor.x.y)

      # Impute missing sd.cfb (using median of correlation)
      sub <- df[is.na(df$sd.cfb),]
      sub$var.cfb <- (sub$sd.b ^2) + (sub$sd.f ^2) +
        (sub$sd.b * sub$sd.f * cor['50%'])

      df$sd.cfb[is.na(df$sd.cfb)] <- sub$var.cfb ^ 0.5

      # Update usecont
      df$usecont <- ifelse(!is.na(df$y.cfb) & !is.na(df$sd.cfb) & !is.na(df$n),
                           "cfb",
                           "f")

    } else {
      stop(paste0("y.cfb studies are missing data on sd.cfb: ", paste0(df$study_id[isna], collapse=", ")))
    }
  }

  message("Number of data points for each outome:")
  print(table(df$usecont))


  # Drop connecting only treatments
  if (connectonly==FALSE) {
    droptrt <- classcode$trt[classcode$connectonly==1]

    ind <- which(df$trt %in% droptrt)

    dropstud <- unique(df$study_id[ind])

    message(paste0(paste(droptrt, collapse=", "), " have been dropped from studies ",
                 paste0(paste(dropstud, collapse=", "))))

    df <- subset(df, !trt %in% droptrt)
  }

  # Drop studies with <2 arms
  df <- df %>% group_by(study_id) %>%
    mutate(arm=seq(n()),
           narms=n())

  drops <- subset(df, narms<2)
  df <- subset(df, narms>=2)
  if (nrow(drops)>0) {
    cat(paste0("The following studies have <2 arms after data cleaning:\n", paste(unique(drops$study_id), collapse="\n")))
  }


  # Add new treatment codes
  bugs.trt <- recode.trt(df=df, classcode=classcode, ref=ref)
  df$trtcode <- bugs.trt$newtcode[match(df$trt, bugs.trt$trt)]


  # Add scale-specific SDs
  df <- df %>% group_by(scale) %>%
    mutate(scalesd=poolsd(n=n, sd=sd.b))

  # Combine (but rescale) VAS 100 and VAS 10
  vas10 <- subset(df, grepl("VAS", scale)) %>%
    mutate(sd10=case_when(scale=="VAS scale (0-10;LB)" ~ sd.b,
                          scale=="VAS scale (0-100;LB)" ~ sd.b/10))

  vas10sd <- poolsd(n=vas10$n, sd=vas10$sd10)

  df <- df %>% mutate(scalesd=case_when(scale=="VAS scale (0-10;LB)" ~ vas10sd,
                                        scale=="VAS scale (0-100;LB)" ~ vas10sd*10,
                                        TRUE ~ scalesd))


  ##### Arrange df (incl reordering arms) and return output #####

  df <- df %>% arrange(study_id, trtcode)

  df <- df %>% group_by(study_id) %>%
    mutate(arm=seq(n()),
           narms=n())

  return(list("data.df"=df, "trtcodes"=bugs.trt,
              "likelihood"="normal"))
}






#' Clean binary data
#'
#' Cleans binary data and generates treatment/class codes required for analysis.
#'
#' @inheritParams cleancont.smd
#' @param r A string corresponding to the variable in `df` for the number of responders (numerator)
#' @param n A string corresponding to the variable in `df` for the total number of participants (denominator).
#' Can be either number randomised or number of completers depending on the desired analysis (ITT or PP)
#'
#' @export
cleanbin <- function(df, classcode,
                     r="discae", n="nrand",
                     connectonly=FALSE, drop.studies=NULL, drop.trt=NULL,
                     connect.study=c(), ref=NULL) {
  # `df` is a data frame of data to be sent to WinBUGS in LONG format
  # study_id: study identifier
  # `classcode` is a data frame of treatment/class identifiers
  # `r` is the substring in `df` corresponding to responders
  # `n` is the substring in `df` corresponding to the denominator (e.g. number randomised)
  # `connectonly` indicates whether treatments only used for connecting (e.g. "Any AD") should be included (TRUE) or not (FALSE)
  # `connect.study` is a vector of study IDs that should be included specifically for network connectivity
  # `ref` is an index for which treatment should become the reference treatment in `classcode`

  # Drop specified studies
  if (!is.null(drop.studies)) {
    df <- df[!df$study %in% c(drop.studies),]
  }


  # Drop specified treatments
  for (i in seq_along(drop.trt)) {
    df <- subset(df, trt!=drop.trt[i])
  }


  # Select outcome
  # Drop studies with missing arm data
  df <- subset(df, !is.na(df[[r]]))
  df <- subset(df, !is.na(df[[n]]))

  # Drop studies with r=0 in trt and active in all 2-arms
  studies <- unique(df$study_id)
  for (i in seq_along(studies)) {

    # If all arms = 0
    if (all(df[[r]]==0)) {
      df <- subset(df, study_id!=studies[i])
      message(paste0("Studies dropped due to zero responders in all arms: ", studies[i]))
    }

    # If all arms r=n
    if (all(df[[r]]==df[[n]])) {
      df <- subset(df, study_id!=studies[i])
      message(paste0("Studies dropped due to r=n in all arms: ", studies[i]))
    }
  }

  # Add new variables for r and n
  df$r <- df[[r]]
  df$n <- df[[n]]



  # Drop connecting only treatments
  if (connectonly==FALSE) {
    droptrt <- classcode$trt[classcode$connectonly==1]

    ind <- which(df$trt %in% droptrt)

    dropstud <- unique(df$study_id[ind])

    message(paste0(paste(droptrt, collapse=", "), " have been dropped from studies ",
                 paste0(paste(dropstud, collapse=", "))))

    df <- subset(df, !trt %in% droptrt)
  }

  # Drop studies with <2 arms
  df <- df %>% group_by(study_id) %>%
    mutate(arm=seq(n()),
           narms=n())

  drops <- subset(df, narms<2)
  df <- subset(df, narms>=2)
  if (nrow(drops)>0) {
    cat(paste0("The following studies have <2 arms after data cleaning:\n", paste(unique(drops$study_id), collapse="\n")))
  }


  # Add new treatment codes
  bugs.trt <- recode.trt(df=df, classcode=classcode, ref=ref)
  df$trtcode <- bugs.trt$newtcode[match(df$trt, bugs.trt$trt)]



  ##### Arrange df (incl reordering arms) and return output #####

  df <- df %>% arrange(study_id, trtcode)

  df <- df %>% group_by(study_id) %>%
    mutate(arm=seq(n()),
           narms=n())

  return(list("data.df"=df, "trtcodes"=bugs.trt,
              "likelihood"="binomial"))
}




#' Returns a data frame of new treatment and class codes with `ref` as the reference treatment
recode.trt <- function(df, classcode, ref=NULL) {

  ###### Recode treatments and classes for analysis #######

  bugs.trt <- data.frame("trt"=unique(df$trt))

  bugs.trt$oldtcode <- classcode$trtcode[match(bugs.trt$trt, classcode$trt)]

  bugs.trt$oldccode <- classcode$classcode[match(bugs.trt$trt, classcode$trt)]

  bugs.trt <- arrange(bugs.trt, oldtcode)

  bugs.trt$newtcode <- 1:nrow(bugs.trt)

  bugs.trt$newccode <- renumber(bugs.trt$oldccode)

  bugs.trt$decision <- classcode$decision[match(bugs.trt$trt, classcode$trt)]

  if (!is.null(ref)) {
    bugs.trt <- rbind(subset(bugs.trt, trt==ref),
                      subset(bugs.trt, trt!=ref))
    bugs.trt$newtcode <- 1:nrow(bugs.trt)
    bugs.trt$newccode <- renumber(bugs.trt$oldccode)
  }

  bugs.trt$class <- classcode$class[match(bugs.trt$trt, classcode$trt)]

  # CURRENTLY NOT IN USE
  # bugs.trt$sharevar <- classcode$sharevar[match(bugs.trt$trt, classcode$trt)]
  # bugs.trt$maxvar <- classcode$maxvar[match(bugs.trt$trt, classcode$trt)]
  bugs.trt$biascat <- classcode$biascat[match(bugs.trt$trt, classcode$trt)]

  return(bugs.trt)
}






# Pools SDs (weighted by n)
# n is a vector of sample sizes
# sd is a vector of sds

#' Pools standard deviations (SD) weighted by `n`
#'
#' @param n A numeric vector of sample sizes
#' @param sd A numeric vector of SDs
#'
#' @export
poolsd <- function(n, sd) {

  if (length(n)!=length(sd)) {
    stop("length(n) must equal length(sd)")
  }

  var <- sd^2
  num <- 0
  denom <- sum(n)-length(n)

  for (i in seq_along(sd)) {
    num <- num + ((n[i]-1)*var[i])
  }

  return((num/denom)^0.5)
}







#' Prepare data for OpenBUGS/WinBUGS
#'
#' @return Returns an object of class `"bugsdat"` that contains all the info required for an analysis in
#' OpenBUGS or WinBUGS.
#'
#' @param cleandat A list of inputs from a function such as cleanbin() or cleancont(). List elements must
#' be named:
#' * `"data.df"` An arm-based data frame of cleaned data
#' * `"trtcodes"` A data frame of treatment/class codes specific to this outcome/analysis
#' * `"likelihood"` A string indicating the likelihood of the data (`"binomial"` or `"continuous"`)
#'
#' @param bias.adjust Indicates whether bias variables should be added to data (`TRUE`) or not (`FALSE`)
#' @param varshare A numeric vector of codes to indicate which classes should share the same class variance
#' in a random class model. Must be same length as number of treatments, and codes must be the same within
#' a class. Codes must be sequentially numbered (starting at 1).
#' @param covariate A string representing the name of a variable in `cleandat$data.df` to be included as
#' a covariate in a Network Meta-Regression. `NULL` indicates no covariate should be included.
#' @param scalesd Indicates whether the variable `scalesd` in `cleandat$data.df` should be used to
#' standardise SD (`TRUE`) rather than using the study-specific SD. E.g. If an internal/external reference SD
#' has been calculated for each scale, then the variable `scalesd` should be included in `cleandat$data.df`.
#' Values for this should be consistent within measurements from the same scale.
#' @param add.continuity Only applicable if `cleandat$likelihood=="binomial"`. A list of study identifiers
#' for which a continuity correction of 0.5 should be added. Values should match those in `cleandat$data.df$study_id`
#' @param add.weight.continuity Only applicable if `cleandat$likelihood=="binomial"`. A list of
#' study identifiers for which a continuity correction using the reciprocal opposite treatment arm
#' weight (Sweeting 2004) should be added. Values should match those in `cleandat$data.df$study_id`
#' @param sd2init Indicates whether initial values for sd2 (within-class effect) should be added for any classes
#' or not. If `FALSE` then this indicates all classes should be modelled with a fixed class effect.
#' @param dinit Indicates whether initial values for d should be added. If `FALSE` then this
#' suggests a fixed class effect should be modelled for all classes.
#' @param deltainit Indicates whether initial values should be added for deltainit. If `FALSE` then this
#' suggests a fixed treatment effect model, whereas `TRUE` suggests a random treatment effects model
#' will be fitted.
#' @param sd2.init.na A numeric vector of class codes to indicate which classes do not need an initial value
#' for sd2 (the within-class SD) - i.e. that they have been modelled with a fixed class effect.
#' @param d.init.na A numeric vector of treatment codes to indicate which treatment effects do not need an initial
#' value - i.e. that they have been modelled with a fixed class effect.
#' @param ume `TRUE` indicates that a UME model will be fitted and therefore the relevant initial values
#' will be generated.
#' @param stroutput If `TRUE` this indicates that data and initial values should be written to a .csv file
#' named `BUGSdata/bugsdata.csv` (i.e. stored in the `BUGSdata` directory). This can then be copied and pasted
#' directly into an OpenBUGS or WinBUGS file so that the model can be run manually by the user rather than
#' through `R2OpenBUGS`/`R2WinBUGS`.
#'
#' @return An list object of class `"bugsdat"` containing the following named elements:
#'
#' * `arraydat` : A wide (one row per study) data frame of BUGS data with the necessary
#' variables for analysis
#' * `trtcodes` : A data frame of treatment names and indices matching the treatment codes
#' in `arraydat`. Can also include corresponding class names and codes for class effect
#' models.
#' * `data.list` : Data for the model saved as a list of objects for the BUGS model in a
#' format that can be used directly with `R2OpenBUGS`/`R2WinBUGS`.
#' * `inits` : A list of initial values in a format that can be used directly with
#' `R2OpenBUGS`/`R2WinBUGS`.
#'
#'
#' @export
prepbugs <- function(cleandat,
                     varshare=c(), bias.adjust=FALSE, covariate=NULL, scalesd=TRUE,
                     add.continuity=list(), add.weight.continuity=list(),
                     sd2.init.na=NULL, d.init.na=NULL,
                     dinit=FALSE, sd2init=FALSE, deltainit=FALSE,
                     ume=FALSE,
                     stroutput=FALSE) {

  # Add additional object for bias
  if (bias.adjust==TRUE) {
    bias <- TRUE
  } else {bias <- FALSE}

  # List elements from cleandat object
  df <- cleandat$data.df
  bugs.trt <- cleandat$trtcodes
  likelihood <- cleandat$likelihood

  if (likelihood=="binomial") {
    if (!"r" %in% names(df)) {
      stop("Binomial likelihood must contain info on r")
    }
    if (!"n" %in% names(df)) {
      stop("Binomial likelihood must contain info on n")
    }
  } else if (likelihood=="normal") {
    message(("Normal likelihood data must contain CFB or BF variables"))
  }


  # Check varshare
  if (length(varshare)>0) {
    if (length(varshare)!=nrow(bugs.trt)) {
      stop("varshare must have same length as number of treatments in cleandat$trtcodes")
    }

    bugs.trt$varshare <- varshare

    # Check varshare is consistent within class coding
    temp <- bugs.trt %>% select(newccode, varshare) %>%
      unique(.)
    if (!all.equal(temp$newccode, 1:nrow(temp))) {
      stop("varshare must be coded the same within a class")
    }

    # Check coding is 1:n
    if (!all.equal(unique(temp$varshare), 1:max(temp$varshare))) {
      stop("varshare must be sequentially numbered (1:max(varshare))")
    }
  }


  ##### Arrange df (incl reordering arms) and return output #####

  df <- df %>% arrange(study_id, trtcode)

  df <- df %>% group_by(study_id) %>%
    mutate(arm=seq(n()),
           narms=n())


  # Maxarm
  maxarm <- max(df$narms)
  narms.df <- df %>% select(study_id, narms) %>%
    group_by(study_id) %>%
    slice(1)

  bugs.df <- data.frame("na"=narms.df$narms)

  for (i in 1:maxarm) {

    t <- ifelse(i<=narms.df$narms,1,0)

    t[t==1] <- df$trtcode[df$arm==i]
    t[t==0] <- NA

    bugs.df[[paste0("t",i)]] <- t

    if (likelihood=="binomial") {
      bugs.df[[paste0("r",i)]] <- rep(NA, length(t))
      bugs.df[[paste0("r",i)]][!is.na(t)] <- df$r[df$arm==i]

    } else if (likelihood=="normal") {

      df <- df %>% arrange(usecont, study_id, arm)

      if ("cfb" %in% df$usecont) {

        bugs.df[[paste0("yCFB",i)]] <- rep(NA, length(t))
        bugs.df[[paste0("sdCFB",i)]] <- rep(NA, length(t))

        nocfb <- nrow(subset(df, arm==i & usecont!="cfb")) # Number of non-y.cfb data points

        bugs.df[[paste0("yCFB",i)]][!is.na(t)] <- c(df$y.cfb[df$arm==i & df$usecont=="cfb"], # y.cfb data
                                                    rep(NA, nocfb))

        bugs.df[[paste0("sdCFB",i)]][!is.na(t)] <- c(df$sd.cfb[df$arm==i & df$usecont=="cfb"], # sd.cfb data
                                                     rep(NA, nocfb))

      }
      if ("f" %in% df$usecont) {

        bugs.df[[paste0("yB",i)]] <- rep(NA, length(t))
        bugs.df[[paste0("yF",i)]] <- rep(NA, length(t))
        bugs.df[[paste0("sdB",i)]] <- rep(NA, length(t))
        bugs.df[[paste0("sdF",i)]] <- rep(NA, length(t))

        nof <- nrow(subset(df, arm==i & usecont!="f")) # Number of non-y.f data points

        bugs.df[[paste0("yF",i)]][!is.na(t)] <- c(rep(NA, nof),
                                                  df$y.f[df$arm==i & df$usecont=="f"] # y.f data
        )

        bugs.df[[paste0("yB",i)]][!is.na(t)] <- c(rep(NA, nof),
                                                  df$y.b[df$arm==i & df$usecont=="f"] # y.b data
        )

        bugs.df[[paste0("sdF",i)]][!is.na(t)] <- c(rep(NA, nof),
                                                   df$sd.f[df$arm==i & df$usecont=="f"] # sd.f data
        )

        bugs.df[[paste0("sdB",i)]][!is.na(t)] <- c(rep(NA, nof),
                                                   df$sd.b[df$arm==i & df$usecont=="f"] # sd.b data
        )
      }

    }

    bugs.df[[paste0("n",i)]] <- rep(NA, length(t))
    bugs.df[[paste0("n",i)]][!is.na(t)] <- df$n[df$arm==i]
  }

  # Reorder columns
  bugs.df <- bugs.df[,c(1,
                        grep("^t", colnames(bugs.df)),
                        grep("^r", colnames(bugs.df)),
                        grep("^n[0-9]", colnames(bugs.df)),

                        grep("^yCFB[0-9]", colnames(bugs.df)),
                        grep("^sdCFB[0-9]", colnames(bugs.df)),

                        grep("^yF[0-9]", colnames(bugs.df)),
                        grep("^sdF[0-9]", colnames(bugs.df)),

                        grep("^yB[0-9]", colnames(bugs.df)),
                        grep("^sdB[0-9]", colnames(bugs.df))
  )]

  # If reference SD used for standardisation
  if (scalesd==TRUE & "scalesd" %in% names(df)) {
    scale.df <- df %>% select(study_id, scalesd) %>%
      group_by(study_id) %>%
      slice(1)

    bugs.df$scalesd <- scale.df$scalesd
  }

  if (!is.null(covariate)) {
    if (bias.adjust==TRUE) {
      stop("Bias-adjustment can only be included if is.null(covariate)")
    }

    cov.df <- df %>% select(study_id, !!as.symbol(covariate)) %>%
      group_by(study_id) %>%
      slice(1)

    bugs.df$x <- cov.df[[covariate]]

    mx <- round(mean(bugs.df$x),2)

  }

  bugs.df <- bugs.df %>% mutate('#'=rep('#', nrow(bugs.df)),
                                studyid = narms.df$study_id
  )

  # Order by number of arms
  if (any(grepl("^t3", names(bugs.df)))) {
    bugs.df <- arrange(bugs.df, desc(na), t1, t2, t3)
  } else {
    bugs.df <- arrange(bugs.df, desc(na), t1, t2)
  }




  ####### Error checking: Check narms matches available data #######

  err.1 <- vector()
  err.2 <- vector()
  for (i in seq_along(bugs.df$na)) {
    narm.1 <- bugs.df$na[i]
    narm.2 <- sum(!is.na(bugs.df[i, paste("t", 1:maxarm, sep="")]))

    if (likelihood=="binomial") {
      narm.3 <- sum(!is.na(bugs.df[i, paste("r", 1:maxarm, sep="")]))

      if (narm.1!=narm.3) {
        err.1 <- append(err.1, bugs.df$studyid[i])
      }
    } else if (likelihood=="normal") {

      if ("cfb" %in% df$usecont) {
        if (!is.na(bugs.df$yCFB1[i])) {
          narm.3 <- sum(!is.na(bugs.df[i, paste("yCFB", 1:maxarm, sep="")]))

          if (narm.1!=narm.3) {
            err.1 <- append(err.1, bugs.df$studyid[i])
          }
        }
      }
      if ("f" %in% df$usecont) {
        if (!is.na(bugs.df$yF1[i])) {
          narm.3 <- sum(!is.na(bugs.df[i, paste("yF", 1:maxarm, sep="")]))

          if (narm.1!=narm.3) {
            err.1 <- append(err.1, bugs.df$studyid[i])
          }
        }
      }

    }

    if (narm.1!=narm.2) {
      err.2 <- append(err.2, bugs.df$studyid[i])
    }
  }
  if (length(err.1)>0) {
    stop(paste("Arms given in `na` not equal to reponders/yCFB/yF for which data are available in studies:\n",
               paste(err.1, collapse="\n")))
  }
  if (length(err.2)>0) {
    stop(paste("Arms given in `na` not equal to treatments for which codes are available in studies:\n",
               paste(err.2, collapse="\n")))
  }


  ####### Continuity corrections ########

  # Add continuity correction
  if (likelihood=="binomial") {
    bugs.df <- add.continuity(bugs.df,
                              add.continuity = add.continuity,
                              add.weight.continuity = add.weight.continuity)
  }

  # Set bugs.df column names
  names(bugs.df) <- gsub("^na", "na[]", names(bugs.df))

  varlist <- c("t", "r", "n", "yCFB", "sdCFB", "yF", "sdF", "yB", "sdB")
  for (i in seq_along(varlist)) {
    names(bugs.df) <- gsub(paste0("^", varlist[i], "([0-9])"),
                           paste0(varlist[i], "[,\\1]"),
                           names(bugs.df))
  }

  names(bugs.df) <- gsub("^scalesd$", "scalesd[]", names(bugs.df))

  if (!is.null(covariate)) {
    names(bugs.df) <- gsub("^x$", "covxR[]", names(bugs.df))
  }

  bugsbits <- addbugsbits(bugs.df, bugs.trt, d.init.na=d.init.na, sd2.init.na=sd2.init.na,
                          ume=ume, covariate=!is.null(covariate), bias.adjust=bias.adjust,
                          dinit=dinit, sd2init=sd2init, deltainit=deltainit,
                          stroutput = stroutput)

  trtcodes <- data.frame("treatcode"=bugs.trt$newtcode, "treat"=bugs.trt$trt,
                         "classcode"=bugs.trt$newccode, "class"=bugs.trt$class,
                         #"group"=bugs.trt$group, "sharevar"=bugs.trt$sharevar, "maxvar"=bugs.trt$maxvar,
                         "biascat"=bugs.trt$biascat
  )

  if (length(varshare)>0) {
    trtcodes$varshare <- bugs.trt$varshare
  }

  if (bias.adjust==TRUE) {
    bugs.df <- bias.adjust(bugs.df, trtcodes)
  }

  # Reorder arms if network reference treatment has changed - MAY BE NECESSARY AFTER biasadjust()
  # if (!is.null(ref)) {
  #   bugs.df <- reorderarray(bugs.df)
  # }

  # Add binomial data to bugsbits if stroutput==FALSE
  if (stroutput==FALSE) {
    bugsbits$data.list$na <- bugs.df$`na[]`
    bugsbits$data.list$t <- as.matrix(bugs.df[,grepl("^t\\[", colnames(bugs.df))])
    bugsbits$data.list$studyid <- bugs.df$studyid

    # If reference SD used for standardisation
    if (any(grepl("scalesd", names(bugs.df)))) {
      bugsbits$data.list$scalesd <- bugs.df$`scalesd[]`
    }

    if (length(varshare)>0) {
      bugsbits$data.list$Vshare <- trtcodes$varshare
      bugsbits$data.list$nVshare <- max(trtcodes$varshare)
    }

    bugsbits$data.list$n <- as.matrix(bugs.df[,grepl("^n\\[", colnames(bugs.df))])

    if (likelihood=="binomial") {
      bugsbits$data.list$r <- as.matrix(bugs.df[,grepl("^r\\[", colnames(bugs.df))])

    } else if (likelihood=="normal") {

      # Consider here whether to have full study length matrix or split into usecont=cfb and usecont=f
      # Currently includes all studies in all matrices

      if (any(grepl("^yCFB", names(bugs.df)))) {
        bugsbits$data.list$yCFB <- as.matrix(bugs.df[,grepl("^yCFB\\[", colnames(bugs.df))])
        bugsbits$data.list$sdCFB <- as.matrix(bugs.df[,grepl("^sdCFB\\[", colnames(bugs.df))])
      }
      if (any(grepl("^yF", names(bugs.df)))) {
        bugsbits$data.list$yF <- as.matrix(bugs.df[,grepl("^yF\\[", colnames(bugs.df))])
        bugsbits$data.list$sdF <- as.matrix(bugs.df[,grepl("^sdF\\[", colnames(bugs.df))])

        bugsbits$data.list$yB <- as.matrix(bugs.df[,grepl("^yB\\[", colnames(bugs.df))])
        bugsbits$data.list$sdB <- as.matrix(bugs.df[,grepl("^sdB\\[", colnames(bugs.df))])
      }
    }

    # Add bias-adjustment direction indicators
    if (any(grepl("C\\[", names(bugs.df)))) {
      bugsbits$data.list$C <- as.matrix(cbind("C[,1]"=rep(NA, nrow(bugs.df)), bugs.df[,grepl("^C\\[", names(bugs.df))]))
    }

    if (!is.null(covariate)) {
      bugsbits$data.list$covxR <- bugs.df[["covxR[]"]]
    }
  }

  if (stroutput==TRUE) {
    out.str <- paste0("# TREATMENT CODES\n\n", format_delim(trtcodes, delim = " "), "\n\n\n\n")
    out.str <- append(out.str, paste0("# DATA\n\n", bugsbits$data.str, "\n\n\n\n"))
    out.str <- append(out.str, paste0(format_delim(bugs.df, delim=" "), "END", "\n\n\n\n"))
    out.str <- append(out.str, paste0("# INITS\n\n", bugsbits$inits))
    write.table(out.str, row.names = FALSE, file="bugsdata.csv", quote=FALSE)
  }

  trtcodes$decision <- bugs.trt$decision

  outlist <- list("arraydat"=bugs.df, "trtcodes"=trtcodes, "data.list"=bugsbits$data.list,
                  "inits"=bugsbits$inits)

  class(outlist) <- "bugsdat"

  return(outlist)
}





#' Adds continuity correction to bugs data frame
#'
#' @param bugs.df A data frame that must have data for binomial likelihod containing the
#' variables `"r"` and `"n"`
#' @inheritParams prepbugs
#'
add.continuity <- function(bugs.df, add.continuity=list(), add.weight.continuity=list()) {
  # `add.continuity` is a list of studyIDs for which a continuity correction of 0.5 should be added
  # `add.weight.continuity` is a list of studyIDs for which a continuity correction using the reciprocal opposite treatment arm weight (Sweeting 2004) should be added

  if (!any(grepl("^r[0-9]", names(bugs.df))) | !any(grepl("^n[0-9]", names(bugs.df)))) {
    stop("'bugs.df' must be a BUGS wide data frame generated from binomial likelihood data with info on r and n")
  }

  if (!all(add.continuity %in% bugs.df$studyid)) {
    stop("`add.continuity` includes studies not in bugs.df")
  }
  for (i in seq_along(add.continuity)) {
    narm <- bugs.df$na[bugs.df$studyid==add.continuity[i]]
    for (k in 1:narm) {
      bugs.df[[paste0("r",k)]][bugs.df$studyid==add.continuity[i]] <-
        bugs.df[[paste0("r",k)]][bugs.df$studyid==add.continuity[i]] + 0.5
      bugs.df[[paste0("n",k)]][bugs.df$studyid==add.continuity[i]] <-
        bugs.df[[paste0("n",k)]][bugs.df$studyid==add.continuity[i]] + 1
    }
  }

  if (!all(add.weight.continuity %in% bugs.df$studyid)) {
    stop("`add.weight.continuity` includes studies not in bugs.df")
  }
  for (i in seq_along(add.weight.continuity)) {
    narm <- bugs.df$na[bugs.df$studyid==add.weight.continuity[i]]
    N <- vector()
    for (k in 1:narm) {
      N <- append(N, bugs.df[[paste0("n",k)]][bugs.df$studyid==add.weight.continuity[i]])
    }
    # Use reciprocal of other treatment arm size weighting
    R <- c(1/max(N[2:length(N)]), rep(1/N[1], length(N)-1))
    R <- (1/sum(R))*R # Set so that the sum of all continuity corrections add up to 1

    R <- vector()
    for (k in seq_along(N)) {
      R <- append(R, 1/sum(N[-k]))
    }
    R <- (1/sum(R))*R # Set so that the sum of all continuity corrections add up to 1

    for (k in 1:narm) {
      bugs.df[[paste0("r",k)]][bugs.df$studyid==add.weight.continuity[i]] <-
        bugs.df[[paste0("r",k)]][bugs.df$studyid==add.weight.continuity[i]] + R[k]

      bugs.df[[paste0("n",k)]][bugs.df$studyid==add.weight.continuity[i]] <-
        bugs.df[[paste0("n",k)]][bugs.df$studyid==add.weight.continuity[i]] + R[k]
    }
  }

  return(bugs.df)

}





#' Generates WinBUGS data and initial values
addbugsbits <- function(bugs.df, bugs.trt, d.init.na=NULL, sd2.init.na=NULL, ume=FALSE,
                        bugs.df.2=NULL, bugs.binom=NULL, # only for depression
                        dinit=FALSE, sd2init=FALSE, deltainit=FALSE, # Should initial values be written for d, sd2 or delta
                        covariate=FALSE, bias.adjust=FALSE,
                        stroutput=FALSE) {
  # stroutput indicates whether output should be as string or list (for use in R2OpenBugs)

  ns <- nrow(bugs.df)

  if (covariate==TRUE) {
    if("covxCFB[]" %in% names(bugs.df)) {
      x <- bugs.df$`covxCFB[]`
    } else if ("covxBF[]" %in% names(bugs.df)) {
      x <- bugs.df$`covxBF[]`
    } else if ("covxR[]" %in% names(bugs.df)) {
      x <- bugs.df$`covxR[]`
    }
  }

  # ONLY USED FOR DEPRESSION
  # if (!is.null(bugs.df.2)) {
  #   nsCFB <- nrow(bugs.df)
  #   nsBF <- nrow(bugs.df.2)
  #   ns <- ns + nrow(bugs.df.2)
  #
  #   if (covariate==TRUE) {
  #     x <- append(x, bugs.df.2$`covxBF[]`)
  #   }
  # }
  # if (!is.null(bugs.binom)) {
  #   nsR <- nrow(bugs.binom)
  #   ns <- ns + nrow(bugs.binom)
  #
  #   if (covariate==TRUE) {
  #     x <- append(x, bugs.binom$`covxR[]`)
  #   }
  # }

  nt <- nrow(bugs.trt)
  nc <- length(unique(bugs.trt$newccode))
  D <- bugs.trt$newccode
  ntR <- nrow(subset(bugs.trt, decision==1))
  ncR <- length(unique(subset(bugs.trt, decision==1)$newccode))
  tRcode <- which(bugs.trt$decision==1)
  cRcode <- unique(bugs.trt$newccode[bugs.trt$decision==1])

  if (covariate==TRUE) {
    mx <- round(mean(x),2)
  }

  if (stroutput==TRUE) {
    data.list <- paste0("list(ns=", ns, ", ",
                        ifelse(is.null(bugs.df.2), "",
                               paste0("nsCFB=", nsCFB, ", ", "nsBF=", nsBF, ", ")),
                        ifelse(is.null(bugs.binom), "",
                               paste0("nsR=", nsR, ", ")),
                        "nt=", nt,
                        ", nc=", nc, ", ",
                        "D=c(", paste(D, collapse=","), "), ",
                        "tRcode=c(", paste(tRcode, collapse=","), "), ",
                        "cRcode=c(", paste(cRcode, collapse=","), "), ",
                        "ntR=", ntR, ", ",
                        "ncR=", ncR
    )

    if (covariate==TRUE) {
      data.str <- paste0(data.str, ", mx=",mx)
    }

    data.str <- paste0(data.str, ")")

  } else {
    data.list <- list(
      ns=ns,
      nt=nt,
      nc=nc,
      D=D)

    if (ume==FALSE) {
      data.list$tRcode <- tRcode
      data.list$cRcode <- cRcode
      data.list$ntR <- ntR
      data.list$ncR <- ncR
    }

    if (!is.null(bugs.df.2)) {
      data.list[["nsCFB"]] <- nsCFB
      data.list[["nsBF"]] <- nsBF
    }
    if (!is.null(bugs.binom)) {
      data.list[["nsR"]] <- nsR
    }

    if (covariate==TRUE) {
      data.list[["mx"]] <- mx
    }
  }


  # Add initial values (for 3 chains)
  inits <- list()
  for (i in 1:3) {
    if (ume==FALSE) {
      d <- c(NA, round(runif(nt-1, -3,3), 0))
      m <- c(NA, round(runif(nc-1, -3,3),0))
    } else if (ume==TRUE) {
      d <- matrix(nrow=nt, ncol=nt)
      d[lower.tri(d)] <- round(runif(sum(lower.tri(d)), -3,3), 0)
      d <- as.vector(d)

      m <- matrix(nrow=nc, ncol=nc)
      m[lower.tri(m)] <- round(runif(sum(lower.tri(m)), -3,3), 0)
      m <- as.vector(m)
    }
    sd <- round(runif(1,0.1,1.5),1)
    mu <- round(runif(ns, -3,3),0)
    sd2 <- round(runif(1,0.1,1.5),1) # Assumes a single within-class variance

    if (deltainit==TRUE) {
      delta <- matrix(nrow=ns, ncol=max(bugs.df$`na[]`))
      for (row in 1:nrow(delta)) {
        delta[row,2:bugs.df$`na[]`[row]] <- round(runif(bugs.df$`na[]`[row]-1, -3,3), 0)
      }
    }

    if (!is.null(sd2.init.na)) {
      sd2[sd2.init.na] <- NA
    }

    if (is.null(d.init.na)) {
      d.init.na <- c(1:nrow(bugs.trt[bugs.trt$trt==1,])) # Only trt=1 vs trt=1 have NA
    }
    if (ume==FALSE) {
      d[d.init.na] <- NA
    }
    if (bias.adjust==TRUE) {
      b <- round(runif(1,-3,3),1)
    }

    # Drop NAs from end of sd2 to avoid splus1000 error
    while (is.na(sd2[length(sd2)])) {
      sd2 <- sd2[1:(length(sd2)-1)]
    }

    if (stroutput==TRUE) {

      inits[[i]] <- paste0("list(",
                           "\nmu= c(", paste(mu, collapse=","), "), ",
                           ifelse(bias.adjust==TRUE, paste0("\nb=", b, ", "), ""),
                           ifelse(sd2init==TRUE, "\n#sd2= c(", paste(sd2, collapse=","), "), ", ""),
                           ifelse(ume==FALSE, paste0(
                             ifelse(dinit==TRUE, "\nd= c(", paste(d, collapse=","), "), ", ""),
                             "\nm= c(", paste(m, collapse=","), "), "),
                             paste0(
                               "\nm=structure(.Data=c(", paste(m, collapse=","), "), .Dim=c(", nc, ",", nc, ")), "
                             )
                           ),
                           ifelse(is.null(bugs.df.2), "\nsd=", "\nsdev="),
                           sd, ")")

    } else {

      inits[[i]] <- list(mu=mu)

      if (sd2init==TRUE) {
        inits[[i]][["sd2"]] <- sd2
      }

      if (deltainit==TRUE) {
        inits[[i]][["delta"]] <- delta
      }

      if (ume==FALSE) {
        if (dinit==TRUE) {
          inits[[i]][["d"]] <- d
        }
        inits[[i]][["m"]] <- m
      } else {

        if (dinit==TRUE) {
          inits[[i]][["d"]] <- matrix(d, ncol=(length(d)^0.5), nrow=(length(d)^0.5), byrow = TRUE)
        }

        inits[[i]][["delta"]] <- delta
        inits[[i]][["m"]] <- matrix(m, ncol=(length(m)^0.5), nrow=(length(m)^0.5), byrow = TRUE)
      }

      if (bias.adjust==TRUE) {
        inits[[i]][["b"]] <- b
      }

      # if (is.null(bugs.df.2)) {
      #   sd <- sd
      # } else {
      #   sdev <- sd
      # }
    }
  }

  if (stroutput==TRUE) {
    init.out <- paste0("# chain 1\n", inits[[1]],
                       "\n\n# chain 2\n", inits[[2]],
                       "\n\n#chain 3\n", inits[[3]])
  } else {
    init.out <- inits
  }

  return(list("data.list"=data.list, "inits"=init.out))
}






#' Converts BUGS wide data to contrasts for each pair of treatments/classes in the dataset
#'
#' @param bugsdat An object of class `"bugsdat"` containing a BUGS-ready dataset in wide format
#' @param level Can take either `"treat"` or `"class"` to indicate that contrast info should be
#' given as pairs of treatments or classes respectively
#'
#' @export
bugstopairs <- function(bugsdat, level="treat") {
  # bugsdat is an object of class 'bugsdat'
  # level can be "treat" or "class"

  if (class(bugsdat)!="bugsdat") {
    stop("'bugsdat' must be an object of class 'bugsdat'")
  }

  trtcodes <- bugsdat$trtcodes
  wide.df <- bugsdat$arraydat

  if ("na[]" %in% names(wide.df)) {
    names(wide.df)[names(wide.df)=="na[]"] <- "narms"
  } else {
    stop("wide.df must have a variable for number of arms - na[]")
  }

  if ("studyid" %in% names(wide.df)) {
    names(wide.df)[names(wide.df)=="studyid"] <- "study"
  } else {
    wide.df$study <- 1:nrow(wide.df)
  }


  # If bugsdat has CFB
  # names(wide.df) <- gsub("CFB", "", names(wide.df))

  names(wide.df) <- gsub("\\[\\,", "", names(wide.df))
  names(wide.df) <- gsub("\\]", "", names(wide.df))

  pairs.df <- widetopairs(wide.df, output = "contrasts")

  pairs.df$t1 <- trtcodes[[level]][match(pairs.df$t1, trtcodes$treatcode)]
  pairs.df$t2 <- trtcodes[[level]][match(pairs.df$t2, trtcodes$treatcode)]

  pairs.df <- pairs.df %>% group_by(paste(t1, t2, sep="_")) %>% mutate(nr=sum(nr),
                                                                       n1=sum(n1),
                                                                       n2=sum(n2))
  pairs.df <- unique(ungroup(pairs.df)) %>%
    select(t1, t2, n1, n2, nr)

  return(pairs.df)
}








#' Converts a wide data frame into contrasts
#'
#' @param wide.df A BUGS-ready data frame in wide format (one row per study) containing `study` and `narms`
#' variables.
#' @param output Can take either `"studies"` to indicate all study-level pairs should be generated, or
#' `"contrasts"` to indicate that only treatment/class level contrasts should be generated.
#'
#' @export
widetopairs <- function(wide.df, output="studies") {
  # `wide.df` is a BUGS frame in wide format (one row per study) with `study` and `narms` variables
  # `output` can take "pairs" for treatment contrasts (one row per contrast), "long" (one row per arm),
  #or "wide" (one row per study)
  # Note that this uses CFB or follow-up data (rather than calculating CFB from BF)

  if (!output %in% c("studies", "contrasts")) {
    stop("'output' must be either studies (one row per within-study comparison) or contrasts (one row per contrast)")
  }

  out.df <- wide.df

  study <- vector()
  t1 <- vector()
  t2 <- vector()

  n1 <- vector()
  n2 <- vector()
  r1 <- vector()
  r2 <- vector()
  y1 <- vector()
  y2 <- vector()
  sd1 <- vector()
  sd2 <- vector()

  for (i in seq_along(out.df$study)) {
    for (k in 1:(out.df$narms[i]-1)) {
      for (m in (k+1):out.df$narms[i]) {
        study <- append(study, out.df$study[i])
        t1 <- append(t1, out.df[[paste0("t", k)]][i])
        t2 <- append(t2, out.df[[paste0("t", m)]][i])

        if (any(grepl("^n[0-9]", names(out.df)))) {
          n1 <- append(n1, out.df[[paste0("n", k)]][i])
          n2 <- append(n2, out.df[[paste0("n", m)]][i])
        }

        if (any(grepl("^r[0-9]", names(out.df)))) {
          r1 <- append(r1, out.df[[paste0("r", k)]][i])
          r2 <- append(r2, out.df[[paste0("r", m)]][i])
        }

        if (any(grepl("^y[A-Z]+[0-9]", names(out.df)))) {

          # Use CFB or F data
          if (!is.na(out.df[[paste0("yCFB", k)]][i])) {
            y1 <- append(y1, out.df[[paste0("yCFB", k)]][i])
            y2 <- append(y2, out.df[[paste0("yCFB", m)]][i])

            sd1 <- append(sd1, out.df[[paste0("sdCFB", k)]][i])
            sd2 <- append(sd2, out.df[[paste0("sdCFB", m)]][i])

          } else{
            y1 <- append(y1, out.df[[paste0("yF", k)]][i])
            y2 <- append(y2, out.df[[paste0("yF", m)]][i])

            sd1 <- append(sd1, out.df[[paste0("sdF", k)]][i])
            sd2 <- append(sd2, out.df[[paste0("sdF", m)]][i])
          }
        }
      }
    }
  }

  pairs.df <- data.frame("study"=study, "t1"=t1, "t2"=t2)

  if (any(grepl("^n[0-9]", names(out.df)))) {
    pairs.df$n1 <- n1
    pairs.df$n2 <- n2
  }

  if (any(grepl("^r[0-9]", names(out.df)))) {
    pairs.df$r1 <- r1
    pairs.df$r2 <- r2

    pairs.df$lor <- log((pairs.df$r2*(pairs.df$n1-pairs.df$r1)) / (pairs.df$r1*(pairs.df$n2-pairs.df$r2)))
    pairs.df$lse <- 1/pairs.df$r2 + 1/(pairs.df$n1-pairs.df$r1) + 1/pairs.df$r1 + 1/(pairs.df$n2-pairs.df$r2)
    pairs.df$l95 <- pairs.df$lor - (1.96*pairs.df$lse)
    pairs.df$u95 <- pairs.df$lor + (1.96*pairs.df$lse)

    pairs.df$comp <- paste(pairs.df$t1, pairs.df$t2, sep="_")
  }

  if (any(grepl("^y[A-Z]+[0-9]", names(out.df)))) {
    pairs.df$y1 <- y1
    pairs.df$y2 <- y2

    pairs.df$sd1 <- sd1
    pairs.df$sd2 <- sd2

    pairs.df$md <- pairs.df$y2 - pairs.df$y1
    pairs.df$mdse <- ((pairs.df$sd1 * pairs.df$n1 + pairs.df$sd2 * pairs.df$n2) / (pairs.df$n1 + pairs.df$n2 - 2)) ^ 0.5
    pairs.df$l95 <- pairs.df$md - (1.96*pairs.df$mdse)
    pairs.df$u95 <- pairs.df$md + (1.96*pairs.df$mdse)

    pairs.df$comp <- paste(pairs.df$t1, pairs.df$t2, sep="_")
  }

  if (output=="contrasts") {
    # Aggregate comparisons
    pairs.df <- pairs.df %>%
      select(t1, t2, n1, n2)

    pairs.df <- pairs.df %>% group_by(paste(t1, t2, sep="_")) %>% mutate(nr=n(),
                                                                         n1=sum(n1),
                                                                         n2=sum(n2)
    )
    pairs.df <- unique(ungroup(pairs.df))
  }

  out.df <- pairs.df

  return(out.df)

}







radian.rescale <- function(x, start=0, direction=1) {
  c.rotate <- function(x) (x + start) %% (2 * pi) * direction
  c.rotate(scales::rescale(x, c(0, 2 * pi), range(x)))
}








# Change reference treatment for results vs a reference (ref is given as treatment NOT as class)
# 'scale' can be "natural" or "log"

#' Change reference treatment for results vs a reference
#'
#' @param ref A character object for the reference **treatment** (not the reference class)
#' @param trtcodes A data frame of treatment/class names and codes
#' @param es A character object corresponding to the name of the node/parameter in WinBUGS
#' for the treatment/class effects vs every other treatment/class
#' @param level Whether `rerefout()` should be at the treatment (`"treat"`) or class (`"class"`)
#' level
#' @param bugsres A data frame of BUGS posterior summaries
#' @inheritParams bugstoexcel
#'
rerefout <- function(ref="Pill placebo", trtcodes, es="or", level="treat", bugsres, model.scale="natural") {
  es <- paste0("^", es)

  if (!any(grepl(paste0("^", es, "\\["), bugsres$node))) {
    stop(paste0(es, " is not a monitored node in bugsres"))
  }

  if (!ref %in% trtcodes$treat) {
    stop("'ref' not in 'trtcodes$treat'")
  }

  if (level=="class") {
    levelcode <- "classcode"
  } else if (level=="treat") {
    levelcode <- "treatcode"
  }

  index <- trtcodes[[levelcode]][which(trtcodes[[level]]==ref)]

  out.res <- subset(bugsres, grepl(es, node))

  upper <- subset(out.res, grepl(paste0(es, "\\[", index, "\\,"), node)) %>%
    # Ensure no indices lower than index are included in upper
    mutate(temp=gsub(paste0(es, "\\[", index, "\\,([0-9]+)\\]"), "\\1", node)) %>%
    mutate(temp=as.numeric(temp)) %>%
    subset(temp>index) %>% select(!temp)

  upper$treat <- unique(trtcodes[[level]][!is.na(trtcodes[[levelcode]]) &
                                            trtcodes[[levelcode]]>index])

  lower <- subset(out.res, grepl(paste0(es, "\\[[0-9]+\\,", index, "\\]"), node))

  # Inverst result
  if (model.scale=="log") {
    lower <- lower %>% mutate(tmedian=exp(-log(median)),
                              tval2.5pc=exp(-log(val97.5pc)),
                              tval97.5pc=exp(-log(val2.5pc))
    ) %>%
      mutate(median=tmedian,
             val2.5pc=tval2.5pc,
             val97.5pc=tval97.5pc
      ) %>% select(!c(tmedian, tval2.5pc, tval97.5pc))
  } else {
    lower <- lower %>% mutate(tmedian=-median,
                              tval2.5pc=-val97.5pc,
                              tval97.5pc=-val2.5pc
    ) %>%
      mutate(median=tmedian,
             val2.5pc=tval2.5pc,
             val97.5pc=tval97.5pc
      ) %>% select(!c(tmedian, tval2.5pc, tval97.5pc))
  }
  lower <- lower %>% mutate(treat = gsub(paste0(es, "\\[([0-9]+)\\,[0-9]+\\]"), "\\1", lower$node)) %>%
    mutate(treat=trtcodes[[level]][which(trtcodes[[levelcode]] %in% treat)])

  codes <- unique(trtcodes[[level]])
  codes <- codes[!is.na(codes)]
  codes <- codes[codes!=trtcodes[[level]][trtcodes$treat==ref]]

  out.res <- rbind(lower, upper) %>%
    mutate(treat=factor(treat, levels=codes))

  # Convert OR to logOR
  if (model.scale=="log") {
    out.res <- out.res %>% mutate(tmedian=log(median),
                                  tval2.5pc=log(val2.5pc),
                                  tval97.5pc=log(val97.5pc)
    ) %>%
      mutate(median=tmedian,
             val2.5pc=tval2.5pc,
             val97.5pc=tval97.5pc
      ) %>% select(!c(tmedian, tval2.5pc, tval97.5pc))
  }

  return(out.res)
}






# Adds bias adjustment columns
# trtcodes must include a column for 'biascat' that is logical indicating if treatment is active (1) or inactive (0)
bias.adjust <- function(arraydat, trtcodes) {
  col <- which(names(arraydat)=="#")
  maxarm <- max(arraydat$`na[]`)

  biasmat <- matrix(nrow=nrow(arraydat), ncol=maxarm-1)
  tmat <- arraydat[,grepl("^t\\[", names(arraydat))]

  for (i in 1:nrow(tmat)) {
    for (k in 2:(maxarm)) {
      if (!is.na(tmat[i,k])) {
        if (trtcodes$biascat[tmat[i,k]]==0 & trtcodes$biascat[tmat[i,1]]==0) {
          biasmat[i,k-1] <- 1
          # } else if ((tmat[i,k]>1 & tmat[i,1]==1) | (tmat[i,k]==1 & tmat[i,1]>1)) {
        } else if ((trtcodes$biascat[tmat[i,k]]==1 & trtcodes$biascat[tmat[i,1]]==0) |
                   (trtcodes$biascat[tmat[i,k]]==0 & trtcodes$biascat[tmat[i,1]]==1)
        ) {
          biasmat[i,k-1] <- 2
        } else if (trtcodes$biascat[tmat[i,k]]==1 & trtcodes$biascat[tmat[i,1]]==1) {
          biasmat[i,k-1] <- 3
        }
      }
    }
  }
  colnames(biasmat) <- paste0("C[,", 2:maxarm, "]")

  arraydat <- cbind(arraydat[,1:col-1], biasmat, arraydat[,col:ncol(arraydat)])
  return(arraydat)
}








